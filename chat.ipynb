{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK PREPROCESSING INPUT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from collections.abc import MutableMapping\n",
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "\n",
    "# üëáÔ∏è add attributes to `collections` module\n",
    "# before you import the package that causes the issue\n",
    "collections.Mapping = collections.abc.Mapping\n",
    "collections.MutableMapping = collections.abc.MutableMapping\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "collections.MutableSet = collections.abc.MutableSet\n",
    "collections.Callable = collections.abc.Callable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    tokenize_word = word_tokenize(sentence)\n",
    "    return tokenize_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bag_of_words(tokenize_sentence, all_words):\n",
    "    bag = np.zeros(len(all_words), dtype = np.float32)\n",
    "\n",
    "    for ind,word in enumerate(all_words):\n",
    "        if word in tokenize_sentence:\n",
    "            bag[ind] = 1.0\n",
    "    return bag\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    " dic_of_words = []\n",
    "def stem_sentence(sentence,stemmer= PorterStemmer()):\n",
    "    \n",
    "    stemmed_sentence = []\n",
    "    ignore_words = [\"!\", \"?\",\",\" , \".\"]\n",
    "    for word in sentence:\n",
    "        if word not in ignore_words:\n",
    "            word = word.lower()\n",
    "            dic_of_words.append(word)\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            stemmed_sentence.append(stemmed_word)\n",
    "    return stemmed_sentence, dic_of_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "vocabulary = []\n",
    "\n",
    "\n",
    "with open (\"intents.json\", 'r') as file:\n",
    "    intents = json.load(file)\n",
    "for intent in intents[\"intents\"]:\n",
    "    tag = intent[\"tag\"]\n",
    "    tags.append(tag)\n",
    "    sentences = intent[\"patterns\"]\n",
    "    for items in sentences:\n",
    "        w = tokenize_sentence(items)\n",
    "        vocabulary.append(w)\n",
    "        w, dic_of_words = stem_sentence(w)\n",
    "        xy.append((w,tag))\n",
    "        all_words.extend(w)\n",
    "\n",
    "\n",
    "ignore_words = [\"!\", \"?\",\",\" , \".\"]\n",
    "set_of_word = [i.lower() for word in vocabulary for i  in word if i not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "\n",
    "with open (\"tags.json\", 'w') as file:\n",
    "    json.dump(tags, file)\n",
    "\n",
    "with open (\"all_words.json\", 'w') as file:\n",
    "    json.dump(all_words, file)\n",
    "with open (\"dic_words.json\", 'w') as file:\n",
    "    json.dump(dic_of_words, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hiiiiii', 'hiiii', 'hey', 'is', 'anyone', 'there', 'hi', 'there', 'hello', 'hey', 'there', 'howdy', 'hola', 'bonjour', 'konnichiwa', 'guten', 'tag', 'ola', 'good', 'morning', 'good', 'afternoon', 'good', 'evening', 'good', 'night', 'bye', 'see', 'you', 'later', 'goodbye', 'au', 'revoir', 'sayonara', 'ok', 'bye', 'bye', 'then', 'fare', 'thee', 'well', 'thanks', 'thank', 'you', 'that', \"'s\", 'helpful', 'thanks', 'for', 'the', 'help', 'than', 'you', 'very', 'much', 'nothing', 'much', 'who', 'are', 'you', 'what', 'are', 'you', 'who', 'you', 'are', 'tell', 'me', 'more', 'about', 'yourself', 'what', 'is', 'your', 'name', 'what', 'should', 'i', 'call', 'you', 'what', \"'s\", 'your', 'name', 'tell', 'me', 'about', 'yourself', 'what', 'can', 'you', 'do', 'who', 'created', 'you', 'how', 'were', 'you', 'made', 'how', 'were', 'you', 'created', 'my', 'name', 'is', 'i', 'am', 'name', 'i', 'go', 'by', 'could', 'you', 'help', 'me', 'give', 'me', 'a', 'hand', 'please', 'can', 'you', 'help', 'what', 'can', 'you', 'do', 'for', 'me', 'i', 'need', 'support', 'i', 'need', 'help', 'support', 'me', 'please', 'i', 'am', 'feeling', 'lonely', 'i', 'am', 'so', 'lonely', 'i', 'feel', 'down', 'i', 'feel', 'sad', 'i', 'am', 'sad', 'i', 'feel', 'so', 'lonely', 'i', 'feel', 'empty', 'i', 'do', \"n't\", 'have', 'anyone', 'i', 'am', 'so', 'stressed', 'out', 'i', 'am', 'so', 'stressed', 'i', 'feel', 'stuck', 'i', 'still', 'feel', 'stressed', 'i', 'am', 'so', 'burned', 'out', 'i', 'feel', 'so', 'worthless', 'no', 'one', 'likes', 'me', 'i', 'ca', \"n't\", 'do', 'anything', 'i', 'am', 'so', 'useless', 'nothing', 'makes', 'sense', 'anymore', 'i', 'ca', \"n't\", 'take', 'it', 'anymore', 'i', 'am', 'so', 'depressed', 'i', 'think', 'i', \"'m\", 'depressed', 'i', 'have', 'depression', 'i', 'feel', 'great', 'today', 'i', 'am', 'happy', 'i', 'feel', 'happy', 'i', \"'m\", 'good', 'cheerful', 'i', \"'m\", 'fine', 'i', 'feel', 'ok', 'oh', 'i', 'see', 'ok', 'okay', 'nice', 'whatever', 'k', 'fine', 'yeah', 'yes', 'no', 'not', 'really', 'i', 'feel', 'so', 'anxious', 'i', \"'m\", 'so', 'anxious', 'because', 'of', 'i', 'do', \"n't\", 'want', 'to', 'talk', 'about', 'it', 'no', 'just', 'stay', 'away', 'i', 'ca', \"n't\", 'bring', 'myself', 'to', 'open', 'up', 'just', 'shut', 'up', 'i', 'have', 'insominia', 'i', 'am', 'suffering', 'from', 'insomnia', 'i', 'ca', \"n't\", 'sleep', 'i', 'have', \"n't\", 'slept', 'for', 'the', 'last', 'days', 'i', 'ca', \"n't\", 'seem', 'to', 'go', 'to', 'sleep', 'i', 'have', \"n't\", 'had', 'proper', 'sleep', 'for', 'the', 'past', 'few', 'days', 'i', \"'m\", 'scared', 'that', 'sounds', 'awful', 'what', 'do', 'i', 'do', 'no', 'i', 'do', \"n't\", 'want', 'to', 'feel', 'this', 'way', 'i', 'am', 'scared', 'for', 'myself', 'my', 'mom', 'died', 'my', 'brother', 'died', 'my', 'dad', 'passed', 'away', 'my', 'sister', 'passed', 'away', 'someone', 'in', 'my', 'family', 'died', 'my', 'friend', 'passed', 'away', 'you', 'do', \"n't\", 'understand', 'me', 'you', \"'re\", 'just', 'some', 'robot', 'how', 'would', 'you', 'know', 'you', 'ca', \"n't\", 'possibly', 'know', 'what', 'i', \"'m\", 'going', 'through', 'you', \"'re\", 'useless', 'you', 'ca', \"n't\", 'help', 'me', 'nobody', 'understands', 'me', 'that', \"'s\", 'all', 'i', 'do', \"n't\", 'have', 'anything', 'more', 'to', 'say', 'nothing', 'else', 'that', \"'s\", 'all', 'i', 'have', 'to', 'say', 'no', 'that', 'would', 'be', 'all', 'i', 'want', 'to', 'kill', 'myself', 'i', \"'ve\", 'thought', 'about', 'killing', 'myself', 'i', 'want', 'to', 'die', 'i', 'am', 'going', 'to', 'kill', 'myself', 'i', 'am', 'going', 'to', 'commit', 'suicide', 'i', 'hate', 'you', 'i', 'do', \"n't\", 'like', 'you', 'i', 'do', \"n't\", 'trust', 'you', 'you', 'hate', 'me', 'i', 'know', 'you', 'hate', 'me', 'you', 'do', \"n't\", 'like', 'me', 'exams', 'friends', 'relationship', 'boyfriend', 'girlfriend', 'family', 'money', 'financial', 'problems', 'tell', 'me', 'a', 'joke', 'tell', 'me', 'another', 'joke', 'you', 'already', 'told', 'me', 'that', 'you', 'mentioned', 'that', 'already', 'why', 'are', 'you', 'repeating', 'yourself', 'what', 'are', 'you', 'saying', 'that', 'does', \"n't\", 'make', 'sense', 'wrong', 'response', 'wrong', 'answer', 'are', 'you', 'stupid', 'you', \"'re\", 'crazy', 'you', 'are', 'dumb', 'are', 'you', 'dumb', 'where', 'are', 'you', 'where', 'do', 'you', 'live', 'what', 'is', 'your', 'location', 'i', 'want', 'to', 'talk', 'about', 'something', 'else', 'let', \"'s\", 'talk', 'about', 'something', 'else', 'can', 'we', 'not', 'talk', 'about', 'this', 'i', 'do', \"n't\", 'want', 'to', 'talk', 'about', 'this', 'i', 'do', \"n't\", 'have', 'any', 'friends', 'can', 'i', 'ask', 'you', 'something', 'probably', 'because', 'my', 'exams', 'are', 'approaching', 'i', 'feel', 'stressed', 'out', 'because', 'i', 'do', \"n't\", 'think', 'i', \"'ve\", 'prepared', 'well', 'enough', 'probably', 'because', 'of', 'my', 'exams', 'i', 'guess', 'not', 'all', 'i', 'can', 'think', 'about', 'are', 'my', 'exams', 'not', 'really', 'i', 'guess', 'not', 'ok', 'sure', 'i', 'would', 'like', 'to', 'learn', 'more', 'about', 'it', 'yes', 'i', 'would', 'like', 'to', 'learn', 'more', 'about', 'it', 'i', 'would', 'like', 'to', 'learn', 'more', 'about', 'it', 'yeah', 'you', \"'re\", 'right', 'i', 'deserve', 'a', 'break', 'yeah', 'you', \"'re\", 'absolutely', 'right', 'about', 'that', 'hmmm', 'that', 'sounds', 'like', 'it', 'could', 'be', 'useful', 'to', 'me', 'that', 'sounds', 'useful', 'i', 'did', 'what', 'you', 'said', 'and', 'i', 'feel', 'alot', 'better', 'thank', 'you', 'very', 'much', 'i', 'feel', 'better', 'now', 'thank', 'you', 'very', 'much', 'again', 'i', \"'ll\", 'continue', 'practicing', 'meditation', 'and', 'focus', 'on', 'what', 'i', 'can', 'control', 'i', 'want', 'some', 'advice', 'i', 'need', 'some', 'advice', 'i', 'need', 'advice', 'on', 'something', 'i', 'want', 'to', 'learn', 'about', 'mental', 'health', 'i', 'want', 'to', 'learn', 'more', 'about', 'mental', 'health', 'i', \"'m\", 'interested', 'in', 'learning', 'about', 'mental', 'health', 'tell', 'me', 'a', 'fact', 'about', 'mental', 'health', 'tell', 'me', 'another', 'fact', 'about', 'mental', 'health', 'what', 'is', 'mental', 'health', 'define', 'mental', 'health', 'why', 'is', 'mental', 'health', 'important', 'what', 'is', 'the', 'importance', 'of', 'mental', 'health', 'what', 'is', 'depression', 'define', 'depression', 'how', 'do', 'i', 'know', 'if', 'i', 'have', 'depression', 'am', 'i', 'depressed', 'am', 'i', 'suffering', 'from', 'depression', 'am', 'i', 'mentally', 'ill', 'what', 'is', 'a', 'therapist', 'what', 'does', 'a', 'therapist', 'do', 'what', 'is', 'therapy', 'do', 'i', 'need', 'therapy', 'who', 'is', 'therapy', 'for', 'what', 'does', 'it', 'mean', 'to', 'have', 'a', 'mental', 'illness', 'who', 'does', 'mental', 'illness', 'affect', 'what', 'causes', 'mental', 'illness', 'what', 'are', 'some', 'of', 'the', 'warning', 'signs', 'of', 'mental', 'illness', 'can', 'people', 'with', 'mental', 'illness', 'recover', 'what', 'should', 'i', 'do', 'if', 'i', 'know', 'someone', 'who', 'appears', 'to', 'have', 'the', 'symptoms', 'of', 'a', 'mental', 'disorder', 'how', 'can', 'i', 'find', 'a', 'mental', 'health', 'professional', 'for', 'myself', 'or', 'my', 'child', 'what', 'treatment', 'options', 'are', 'available', 'if', 'i', 'become', 'involved', 'in', 'treatment', 'what', 'do', 'i', 'need', 'to', 'know', 'what', 'is', 'the', 'difference', 'between', 'mental', 'health', 'professionals', 'how', 'can', 'i', 'find', 'a', 'mental', 'health', 'professional', 'right', 'for', 'my', 'child', 'or', 'myself', 'where', 'else', 'can', 'i', 'get', 'help', 'what', 'should', 'i', 'know', 'before', 'starting', 'a', 'new', 'medication', 'where', 'can', 'i', 'go', 'to', 'find', 'therapy', 'where', 'can', 'i', 'learn', 'about', 'types', 'of', 'mental', 'health', 'treatment', 'what', 'are', 'the', 'different', 'types', 'of', 'mental', 'health', 'professionals', 'where', 'can', 'i', 'go', 'to', 'find', 'a', 'support', 'group', 'can', 'you', 'prevent', 'mental', 'health', 'problems', 'are', 'there', 'cures', 'for', 'mental', 'health', 'problems', 'is', 'there', 'any', 'cure', 'for', 'mental', 'health', 'problems', 'what', 'causes', 'mental', 'health', 'problems', 'what', 'do', 'i', 'do', 'if', 'i', \"'m\", 'worried', 'about', 'my', 'mental', 'health', 'how', 'do', 'i', 'know', 'if', 'i', \"'m\", 'unwell', 'how', 'can', 'i', 'maintain', 'social', 'connections', 'what', 'if', 'i', 'feel', 'lonely', 'what', \"'s\", 'the', 'difference', 'between', 'anxiety', 'and', 'stress', 'what', \"'s\", 'the', 'difference', 'between', 'sadness', 'and', 'depression', 'difference', 'between', 'sadness', 'and', 'depression']\n"
     ]
    }
   ],
   "source": [
    "print(set_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (sequence, tag )in xy:\n",
    "    bag = bag_of_words(sequence, all_words)\n",
    "    train_x.append(bag)\n",
    "\n",
    "    label = tags.index(tag)\n",
    "    train_y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainy one hot encoding\n",
    "hot_output = []\n",
    "output_code = [0 for i in range(len(tags))]\n",
    "for tag_no in train_y:\n",
    "    output_tag = output_code[:]\n",
    "    output_tag[tag_no] = 1\n",
    "    hot_output.append(output_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "train_y = np.array(hot_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234, 281)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(128, activation = \"relu\", input_shape =(None, train_x.shape[0], train_x.shape[1]) ))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(64,activation = \"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(len(tags), activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 2s 5ms/step - loss: 4.3995 - accuracy: 0.0043\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4.3767 - accuracy: 0.0214\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.3530 - accuracy: 0.0214\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.3197 - accuracy: 0.0427\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.3158 - accuracy: 0.0385\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4.2964 - accuracy: 0.0513\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.2615 - accuracy: 0.0556\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.2383 - accuracy: 0.0940\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.2025 - accuracy: 0.1111\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.1761 - accuracy: 0.1282\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4.1467 - accuracy: 0.1068\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4.1075 - accuracy: 0.1282\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4.0878 - accuracy: 0.1154\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.0409 - accuracy: 0.1026\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 4.0097 - accuracy: 0.1282\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.9571 - accuracy: 0.1154\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.8939 - accuracy: 0.1325\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.8443 - accuracy: 0.1410\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.7722 - accuracy: 0.1325\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.7826 - accuracy: 0.1581\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.7488 - accuracy: 0.1667\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3.6498 - accuracy: 0.1453\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3.5916 - accuracy: 0.2009\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3.5377 - accuracy: 0.1880\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.5122 - accuracy: 0.1923\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 3.4107 - accuracy: 0.2265\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.3689 - accuracy: 0.2051\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.4133 - accuracy: 0.1838\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 3.2635 - accuracy: 0.2137\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.2136 - accuracy: 0.2137\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.2151 - accuracy: 0.2564\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.0753 - accuracy: 0.2564\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.1042 - accuracy: 0.2650\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.0875 - accuracy: 0.2564\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.9610 - accuracy: 0.2778\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.9387 - accuracy: 0.2949\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.8674 - accuracy: 0.2821\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.7768 - accuracy: 0.3376\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.7229 - accuracy: 0.2949\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.7316 - accuracy: 0.3248\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.6532 - accuracy: 0.3846\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.5710 - accuracy: 0.3675\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.5235 - accuracy: 0.3761\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.6029 - accuracy: 0.3632\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.3720 - accuracy: 0.4359\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.4187 - accuracy: 0.4231\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.4649 - accuracy: 0.3504\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.3949 - accuracy: 0.3803\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.2114 - accuracy: 0.4615\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.2721 - accuracy: 0.4188\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.1603 - accuracy: 0.4359\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.1237 - accuracy: 0.4744\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.1043 - accuracy: 0.5128\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.0721 - accuracy: 0.5085\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 2.0779 - accuracy: 0.4744\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.9987 - accuracy: 0.4915\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.0366 - accuracy: 0.4487\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.0131 - accuracy: 0.4957\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.9135 - accuracy: 0.4915\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.9255 - accuracy: 0.5043\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.8579 - accuracy: 0.5256\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.7641 - accuracy: 0.5897\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.8485 - accuracy: 0.5085\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7931 - accuracy: 0.4957\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7207 - accuracy: 0.5556\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.7149 - accuracy: 0.5385\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.7108 - accuracy: 0.5726\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.6218 - accuracy: 0.5769\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.6140 - accuracy: 0.5983\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.5580 - accuracy: 0.5940\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.5699 - accuracy: 0.5940\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.6032 - accuracy: 0.5897\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.5536 - accuracy: 0.5684\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.4992 - accuracy: 0.6453\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.5520 - accuracy: 0.5684\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 4ms/step - loss: 1.4583 - accuracy: 0.6325\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.4453 - accuracy: 0.5812\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.4164 - accuracy: 0.6368\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.3461 - accuracy: 0.6026\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.3427 - accuracy: 0.6154\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.4014 - accuracy: 0.6368\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.4219 - accuracy: 0.5897\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.3150 - accuracy: 0.6581\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.2968 - accuracy: 0.6880\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3984 - accuracy: 0.5983\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.2169 - accuracy: 0.6752\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3228 - accuracy: 0.6667\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.2843 - accuracy: 0.6538\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1667 - accuracy: 0.7009\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.1135 - accuracy: 0.7009\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1194 - accuracy: 0.6966\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1514 - accuracy: 0.7137\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1843 - accuracy: 0.6752\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1474 - accuracy: 0.7009\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1313 - accuracy: 0.7094\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1680 - accuracy: 0.6838\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1803 - accuracy: 0.6624\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0924 - accuracy: 0.7179\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0646 - accuracy: 0.7222\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0901 - accuracy: 0.7137\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0357 - accuracy: 0.7222\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.9229 - accuracy: 0.7863\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.0858 - accuracy: 0.7137\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.0777 - accuracy: 0.7222\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9579 - accuracy: 0.7650\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.9709 - accuracy: 0.7564\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.0245 - accuracy: 0.7137\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.0231 - accuracy: 0.7350\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8993 - accuracy: 0.7863\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.0033 - accuracy: 0.7137\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.9588 - accuracy: 0.7778\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.0164 - accuracy: 0.7265\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9571 - accuracy: 0.7479\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8986 - accuracy: 0.7607\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8947 - accuracy: 0.7521\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.9178 - accuracy: 0.7393\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7938 - accuracy: 0.7863\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8231 - accuracy: 0.8162\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8312 - accuracy: 0.7778\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8861 - accuracy: 0.7436\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8714 - accuracy: 0.7692\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8286 - accuracy: 0.7778\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8925 - accuracy: 0.7479\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8766 - accuracy: 0.7863\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8616 - accuracy: 0.7308\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7967 - accuracy: 0.7735\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8105 - accuracy: 0.7650\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7626 - accuracy: 0.7479\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.7164 - accuracy: 0.8205\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7679 - accuracy: 0.7821\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7652 - accuracy: 0.7949\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7982 - accuracy: 0.8120\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8853 - accuracy: 0.7564\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7500 - accuracy: 0.7949\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8123 - accuracy: 0.7949\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7137 - accuracy: 0.8120\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6732 - accuracy: 0.7906\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7523 - accuracy: 0.8376\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7205 - accuracy: 0.8333\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7864 - accuracy: 0.7778\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7755 - accuracy: 0.7863\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7402 - accuracy: 0.8120\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7597 - accuracy: 0.7821\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6883 - accuracy: 0.8333\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7255 - accuracy: 0.7607\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6797 - accuracy: 0.8077\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6858 - accuracy: 0.8205\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6312 - accuracy: 0.8333\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.7430 - accuracy: 0.8120\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5816 - accuracy: 0.8462\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6467 - accuracy: 0.8034\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5672 - accuracy: 0.8675\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5871 - accuracy: 0.8761\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5887 - accuracy: 0.8419\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6517 - accuracy: 0.8419\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6288 - accuracy: 0.8333\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6865 - accuracy: 0.8205\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6769 - accuracy: 0.8077\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7391 - accuracy: 0.7991\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6667 - accuracy: 0.8291\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6084 - accuracy: 0.8120\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5338 - accuracy: 0.8761\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6773 - accuracy: 0.8162\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.7540 - accuracy: 0.7650\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6387 - accuracy: 0.8376\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6371 - accuracy: 0.8162\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6654 - accuracy: 0.8333\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6176 - accuracy: 0.8077\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6933 - accuracy: 0.7692\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6042 - accuracy: 0.8504\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5800 - accuracy: 0.8205\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6119 - accuracy: 0.8291\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5724 - accuracy: 0.8846\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5370 - accuracy: 0.8547\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5984 - accuracy: 0.8162\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5531 - accuracy: 0.8718\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4755 - accuracy: 0.8803\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4831 - accuracy: 0.8675\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6012 - accuracy: 0.8333\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5895 - accuracy: 0.8205\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6140 - accuracy: 0.8376\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6073 - accuracy: 0.8291\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5646 - accuracy: 0.8333\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5283 - accuracy: 0.8718\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5242 - accuracy: 0.8632\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5648 - accuracy: 0.8205\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4805 - accuracy: 0.8803\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5086 - accuracy: 0.8761\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6085 - accuracy: 0.8077\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5315 - accuracy: 0.8590\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6278 - accuracy: 0.8205\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5181 - accuracy: 0.8632\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5476 - accuracy: 0.8291\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5674 - accuracy: 0.8205\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5746 - accuracy: 0.8333\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4926 - accuracy: 0.8889\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4702 - accuracy: 0.8846\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4425 - accuracy: 0.8761\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.4595 - accuracy: 0.8761\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.5568 - accuracy: 0.8248\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eac8696e50>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs = 200, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model2\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model2\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model2\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
