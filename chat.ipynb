{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLTK PREPROCESSING INPUT"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from collections.abc import MutableMapping\n",
    "import streamlit as st\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections.abc\n",
    "\n",
    "# üëáÔ∏è add attributes to `collections` module\n",
    "# before you import the package that causes the issue\n",
    "collections.Mapping = collections.abc.Mapping\n",
    "collections.MutableMapping = collections.abc.MutableMapping\n",
    "collections.Iterable = collections.abc.Iterable\n",
    "collections.MutableSet = collections.abc.MutableSet\n",
    "collections.Callable = collections.abc.Callable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentence(sentence):\n",
    "    tokenize_word = word_tokenize(sentence)\n",
    "    return tokenize_word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bag_of_words(tokenize_sentence, all_words):\n",
    "    bag = np.zeros(len(all_words), dtype = np.float32)\n",
    "\n",
    "    for ind,word in enumerate(all_words):\n",
    "        if word in tokenize_sentence:\n",
    "            bag[ind] = 1.0\n",
    "    return bag\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    " dic_of_words = []\n",
    "def stem_sentence(sentence,stemmer= PorterStemmer()):\n",
    "    \n",
    "    stemmed_sentence = []\n",
    "    ignore_words = [\"!\", \"?\",\",\" , \".\"]\n",
    "    for word in sentence:\n",
    "        if word not in ignore_words:\n",
    "            word = word.lower()\n",
    "            dic_of_words.append(word)\n",
    "            stemmed_word = stemmer.stem(word)\n",
    "            stemmed_sentence.append(stemmed_word)\n",
    "    return stemmed_sentence, dic_of_words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "tags = []\n",
    "xy = []\n",
    "vocabulary = []\n",
    "\n",
    "\n",
    "with open (\"intents.json\", 'r') as file:\n",
    "    intents = json.load(file)\n",
    "for intent in intents[\"intents\"]:\n",
    "    tag = intent[\"tag\"]\n",
    "    tags.append(tag)\n",
    "    sentences = intent[\"patterns\"]\n",
    "    for items in sentences:\n",
    "        w = tokenize_sentence(items)\n",
    "        vocabulary.append(w)\n",
    "        w, dic_of_words = stem_sentence(w)\n",
    "        xy.append((w,tag))\n",
    "        all_words.extend(w)\n",
    "\n",
    "\n",
    "ignore_words = [\"!\", \"?\",\",\" , \".\"]\n",
    "set_of_word = [i.lower() for word in vocabulary for i  in word if i not in ignore_words]\n",
    "all_words = sorted(set(all_words))\n",
    "tags = sorted(set(tags))\n",
    "train_x = []\n",
    "train_y = []\n",
    "\n",
    "\n",
    "with open (\"tags.json\", 'w') as file:\n",
    "    json.dump(tags, file)\n",
    "\n",
    "with open (\"all_words.json\", 'w') as file:\n",
    "    json.dump(all_words, file)\n",
    "with open (\"dic_words.json\", 'w') as file:\n",
    "    json.dump(dic_of_words, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'hiiiiii', 'hiiii', 'hey', 'is', 'anyone', 'there', 'hi', 'there', 'hello', 'hey', 'there', 'howdy', 'hola', 'bonjour', 'konnichiwa', 'guten', 'tag', 'ola', 'good', 'morning', 'good', 'afternoon', 'good', 'evening', 'good', 'night', 'bye', 'see', 'you', 'later', 'goodbye', 'au', 'revoir', 'sayonara', 'ok', 'bye', 'bye', 'then', 'fare', 'thee', 'well', 'thanks', 'thank', 'you', 'that', \"'s\", 'helpful', 'thanks', 'for', 'the', 'help', 'than', 'you', 'very', 'much', 'nothing', 'much', 'who', 'are', 'you', 'what', 'are', 'you', 'who', 'you', 'are', 'tell', 'me', 'more', 'about', 'yourself', 'what', 'is', 'your', 'name', 'what', 'should', 'i', 'call', 'you', 'what', \"'s\", 'your', 'name', 'tell', 'me', 'about', 'yourself', 'what', 'can', 'you', 'do', 'who', 'created', 'you', 'how', 'were', 'you', 'made', 'how', 'were', 'you', 'created', 'my', 'name', 'is', 'i', 'am', 'name', 'i', 'go', 'by', 'could', 'you', 'help', 'me', 'give', 'me', 'a', 'hand', 'please', 'can', 'you', 'help', 'what', 'can', 'you', 'do', 'for', 'me', 'i', 'need', 'support', 'i', 'need', 'help', 'support', 'me', 'please', 'i', 'am', 'feeling', 'lonely', 'i', 'am', 'so', 'lonely', 'i', 'feel', 'down', 'i', 'feel', 'sad', 'i', 'am', 'sad', 'i', 'feel', 'so', 'lonely', 'i', 'feel', 'empty', 'i', 'do', \"n't\", 'have', 'anyone', 'i', 'am', 'so', 'stressed', 'out', 'i', 'am', 'so', 'stressed', 'i', 'feel', 'stuck', 'i', 'still', 'feel', 'stressed', 'i', 'am', 'so', 'burned', 'out', 'i', 'feel', 'so', 'worthless', 'no', 'one', 'likes', 'me', 'i', 'ca', \"n't\", 'do', 'anything', 'i', 'am', 'so', 'useless', 'nothing', 'makes', 'sense', 'anymore', 'i', 'ca', \"n't\", 'take', 'it', 'anymore', 'i', 'am', 'so', 'depressed', 'i', 'think', 'i', \"'m\", 'depressed', 'i', 'have', 'depression', 'i', 'feel', 'great', 'today', 'i', 'am', 'happy', 'i', 'feel', 'happy', 'i', \"'m\", 'good', 'cheerful', 'i', \"'m\", 'fine', 'i', 'feel', 'ok', 'oh', 'i', 'see', 'ok', 'okay', 'nice', 'whatever', 'k', 'fine', 'yeah', 'yes', 'no', 'not', 'really', 'i', 'feel', 'so', 'anxious', 'i', \"'m\", 'so', 'anxious', 'because', 'of', 'i', 'do', \"n't\", 'want', 'to', 'talk', 'about', 'it', 'no', 'just', 'stay', 'away', 'i', 'ca', \"n't\", 'bring', 'myself', 'to', 'open', 'up', 'just', 'shut', 'up', 'i', 'have', 'insominia', 'i', 'am', 'suffering', 'from', 'insomnia', 'i', 'ca', \"n't\", 'sleep', 'i', 'have', \"n't\", 'slept', 'for', 'the', 'last', 'days', 'i', 'ca', \"n't\", 'seem', 'to', 'go', 'to', 'sleep', 'i', 'have', \"n't\", 'had', 'proper', 'sleep', 'for', 'the', 'past', 'few', 'days', 'i', \"'m\", 'scared', 'that', 'sounds', 'awful', 'what', 'do', 'i', 'do', 'no', 'i', 'do', \"n't\", 'want', 'to', 'feel', 'this', 'way', 'i', 'am', 'scared', 'for', 'myself', 'my', 'mom', 'died', 'my', 'brother', 'died', 'my', 'dad', 'passed', 'away', 'my', 'sister', 'passed', 'away', 'someone', 'in', 'my', 'family', 'died', 'my', 'friend', 'passed', 'away', 'you', 'do', \"n't\", 'understand', 'me', 'you', \"'re\", 'just', 'some', 'robot', 'how', 'would', 'you', 'know', 'you', 'ca', \"n't\", 'possibly', 'know', 'what', 'i', \"'m\", 'going', 'through', 'you', \"'re\", 'useless', 'you', 'ca', \"n't\", 'help', 'me', 'nobody', 'understands', 'me', 'that', \"'s\", 'all', 'i', 'do', \"n't\", 'have', 'anything', 'more', 'to', 'say', 'nothing', 'else', 'that', \"'s\", 'all', 'i', 'have', 'to', 'say', 'no', 'that', 'would', 'be', 'all', 'i', 'want', 'to', 'kill', 'myself', 'i', \"'ve\", 'thought', 'about', 'killing', 'myself', 'i', 'want', 'to', 'die', 'i', 'am', 'going', 'to', 'kill', 'myself', 'i', 'am', 'going', 'to', 'commit', 'suicide', 'i', 'hate', 'you', 'i', 'do', \"n't\", 'like', 'you', 'i', 'do', \"n't\", 'trust', 'you', 'you', 'hate', 'me', 'i', 'know', 'you', 'hate', 'me', 'you', 'do', \"n't\", 'like', 'me', 'exams', 'friends', 'relationship', 'boyfriend', 'girlfriend', 'family', 'money', 'financial', 'problems', 'tell', 'me', 'a', 'joke', 'tell', 'me', 'another', 'joke', 'you', 'already', 'told', 'me', 'that', 'you', 'mentioned', 'that', 'already', 'why', 'are', 'you', 'repeating', 'yourself', 'what', 'are', 'you', 'saying', 'that', 'does', \"n't\", 'make', 'sense', 'wrong', 'response', 'wrong', 'answer', 'are', 'you', 'stupid', 'you', \"'re\", 'crazy', 'you', 'are', 'dumb', 'are', 'you', 'dumb', 'where', 'are', 'you', 'where', 'do', 'you', 'live', 'what', 'is', 'your', 'location', 'i', 'want', 'to', 'talk', 'about', 'something', 'else', 'let', \"'s\", 'talk', 'about', 'something', 'else', 'can', 'we', 'not', 'talk', 'about', 'this', 'i', 'do', \"n't\", 'want', 'to', 'talk', 'about', 'this', 'i', 'do', \"n't\", 'have', 'any', 'friends', 'can', 'i', 'ask', 'you', 'something', 'probably', 'because', 'my', 'exams', 'are', 'approaching', 'i', 'feel', 'stressed', 'out', 'because', 'i', 'do', \"n't\", 'think', 'i', \"'ve\", 'prepared', 'well', 'enough', 'probably', 'because', 'of', 'my', 'exams', 'i', 'guess', 'not', 'all', 'i', 'can', 'think', 'about', 'are', 'my', 'exams', 'not', 'really', 'i', 'guess', 'not', 'ok', 'sure', 'i', 'would', 'like', 'to', 'learn', 'more', 'about', 'it', 'yes', 'i', 'would', 'like', 'to', 'learn', 'more', 'about', 'it', 'i', 'would', 'like', 'to', 'learn', 'more', 'about', 'it', 'yeah', 'you', \"'re\", 'right', 'i', 'deserve', 'a', 'break', 'yeah', 'you', \"'re\", 'absolutely', 'right', 'about', 'that', 'hmmm', 'that', 'sounds', 'like', 'it', 'could', 'be', 'useful', 'to', 'me', 'that', 'sounds', 'useful', 'i', 'did', 'what', 'you', 'said', 'and', 'i', 'feel', 'alot', 'better', 'thank', 'you', 'very', 'much', 'i', 'feel', 'better', 'now', 'thank', 'you', 'very', 'much', 'again', 'i', \"'ll\", 'continue', 'practicing', 'meditation', 'and', 'focus', 'on', 'what', 'i', 'can', 'control', 'i', 'want', 'some', 'advice', 'i', 'need', 'some', 'advice', 'i', 'need', 'advice', 'on', 'something', 'i', 'want', 'to', 'learn', 'about', 'mental', 'health', 'i', 'want', 'to', 'learn', 'more', 'about', 'mental', 'health', 'i', \"'m\", 'interested', 'in', 'learning', 'about', 'mental', 'health', 'tell', 'me', 'a', 'fact', 'about', 'mental', 'health', 'tell', 'me', 'another', 'fact', 'about', 'mental', 'health', 'what', 'is', 'mental', 'health', 'define', 'mental', 'health', 'why', 'is', 'mental', 'health', 'important', 'what', 'is', 'the', 'importance', 'of', 'mental', 'health', 'what', 'is', 'depression', 'define', 'depression', 'how', 'do', 'i', 'know', 'if', 'i', 'have', 'depression', 'am', 'i', 'depressed', 'am', 'i', 'suffering', 'from', 'depression', 'am', 'i', 'mentally', 'ill', 'what', 'is', 'a', 'therapist', 'what', 'does', 'a', 'therapist', 'do', 'what', 'is', 'therapy', 'do', 'i', 'need', 'therapy', 'who', 'is', 'therapy', 'for', 'what', 'does', 'it', 'mean', 'to', 'have', 'a', 'mental', 'illness', 'who', 'does', 'mental', 'illness', 'affect', 'what', 'causes', 'mental', 'illness', 'what', 'are', 'some', 'of', 'the', 'warning', 'signs', 'of', 'mental', 'illness', 'can', 'people', 'with', 'mental', 'illness', 'recover', 'what', 'should', 'i', 'do', 'if', 'i', 'know', 'someone', 'who', 'appears', 'to', 'have', 'the', 'symptoms', 'of', 'a', 'mental', 'disorder', 'how', 'can', 'i', 'find', 'a', 'mental', 'health', 'professional', 'for', 'myself', 'or', 'my', 'child', 'what', 'treatment', 'options', 'are', 'available', 'if', 'i', 'become', 'involved', 'in', 'treatment', 'what', 'do', 'i', 'need', 'to', 'know', 'what', 'is', 'the', 'difference', 'between', 'mental', 'health', 'professionals', 'how', 'can', 'i', 'find', 'a', 'mental', 'health', 'professional', 'right', 'for', 'my', 'child', 'or', 'myself', 'where', 'else', 'can', 'i', 'get', 'help', 'what', 'should', 'i', 'know', 'before', 'starting', 'a', 'new', 'medication', 'where', 'can', 'i', 'go', 'to', 'find', 'therapy', 'where', 'can', 'i', 'learn', 'about', 'types', 'of', 'mental', 'health', 'treatment', 'what', 'are', 'the', 'different', 'types', 'of', 'mental', 'health', 'professionals', 'where', 'can', 'i', 'go', 'to', 'find', 'a', 'support', 'group', 'can', 'you', 'prevent', 'mental', 'health', 'problems', 'are', 'there', 'cures', 'for', 'mental', 'health', 'problems', 'is', 'there', 'any', 'cure', 'for', 'mental', 'health', 'problems', 'what', 'causes', 'mental', 'health', 'problems', 'what', 'do', 'i', 'do', 'if', 'i', \"'m\", 'worried', 'about', 'my', 'mental', 'health', 'how', 'do', 'i', 'know', 'if', 'i', \"'m\", 'unwell', 'how', 'can', 'i', 'maintain', 'social', 'connections', 'what', 'if', 'i', 'feel', 'lonely', 'what', \"'s\", 'the', 'difference', 'between', 'anxiety', 'and', 'stress', 'what', \"'s\", 'the', 'difference', 'between', 'sadness', 'and', 'depression', 'difference', 'between', 'sadness', 'and', 'depression']\n"
     ]
    }
   ],
   "source": [
    "print(set_of_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (sequence, tag )in xy:\n",
    "    bag = bag_of_words(sequence, all_words)\n",
    "    train_x.append(bag)\n",
    "\n",
    "    label = tags.index(tag)\n",
    "    train_y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainy one hot encoding\n",
    "hot_output = []\n",
    "output_code = [0 for i in range(len(tags))]\n",
    "for tag_no in train_y:\n",
    "    output_tag = output_code[:]\n",
    "    output_tag[tag_no] = 1\n",
    "    hot_output.append(output_tag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = np.array(train_x)\n",
    "train_y = np.array(hot_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(234, 281)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(tf.keras.layers.Dense(128, activation = \"relu\", input_shape =(None, train_x.shape[0], train_x.shape[1]) ))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(64,activation = \"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.5))\n",
    "model.add(tf.keras.layers.Dense(len(tags), activation = \"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = \"adam\", loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "5/5 [==============================] - 21s 12ms/step - loss: 4.3978 - accuracy: 0.0171\n",
      "Epoch 2/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4.3661 - accuracy: 0.0214\n",
      "Epoch 3/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 4.3321 - accuracy: 0.0598\n",
      "Epoch 4/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 4.3251 - accuracy: 0.0342\n",
      "Epoch 5/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.2974 - accuracy: 0.0470\n",
      "Epoch 6/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 4.2685 - accuracy: 0.0684\n",
      "Epoch 7/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.2572 - accuracy: 0.0726\n",
      "Epoch 8/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 4.2101 - accuracy: 0.0812\n",
      "Epoch 9/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 4.1527 - accuracy: 0.1111\n",
      "Epoch 10/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 4.1535 - accuracy: 0.1068\n",
      "Epoch 11/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 4.1007 - accuracy: 0.1197\n",
      "Epoch 12/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 4.0668 - accuracy: 0.1026\n",
      "Epoch 13/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 4.0584 - accuracy: 0.0855\n",
      "Epoch 14/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 4.0145 - accuracy: 0.1325\n",
      "Epoch 15/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.9445 - accuracy: 0.1325\n",
      "Epoch 16/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.9181 - accuracy: 0.1068\n",
      "Epoch 17/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.8137 - accuracy: 0.1538\n",
      "Epoch 18/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3.7762 - accuracy: 0.1453\n",
      "Epoch 19/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 3.7590 - accuracy: 0.1197\n",
      "Epoch 20/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.7142 - accuracy: 0.1581\n",
      "Epoch 21/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.6712 - accuracy: 0.1239\n",
      "Epoch 22/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3.5972 - accuracy: 0.1923\n",
      "Epoch 23/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3.6151 - accuracy: 0.1496\n",
      "Epoch 24/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 3.5143 - accuracy: 0.1709\n",
      "Epoch 25/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3.4388 - accuracy: 0.1667\n",
      "Epoch 26/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.3537 - accuracy: 0.2222\n",
      "Epoch 27/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 3.4021 - accuracy: 0.2137\n",
      "Epoch 28/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.2740 - accuracy: 0.2650\n",
      "Epoch 29/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 3.1886 - accuracy: 0.2906\n",
      "Epoch 30/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 3.1838 - accuracy: 0.2692\n",
      "Epoch 31/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.1444 - accuracy: 0.2265\n",
      "Epoch 32/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 3.0623 - accuracy: 0.2393\n",
      "Epoch 33/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 3.0473 - accuracy: 0.2778\n",
      "Epoch 34/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 3.0228 - accuracy: 0.2778\n",
      "Epoch 35/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.9279 - accuracy: 0.3120\n",
      "Epoch 36/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.8552 - accuracy: 0.3162\n",
      "Epoch 37/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.7809 - accuracy: 0.3590\n",
      "Epoch 38/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.7810 - accuracy: 0.3162\n",
      "Epoch 39/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.7431 - accuracy: 0.3077\n",
      "Epoch 40/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 2.6046 - accuracy: 0.3547\n",
      "Epoch 41/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.5715 - accuracy: 0.3632\n",
      "Epoch 42/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.5260 - accuracy: 0.3846\n",
      "Epoch 43/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.5182 - accuracy: 0.3547\n",
      "Epoch 44/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 2.4627 - accuracy: 0.4017\n",
      "Epoch 45/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.4223 - accuracy: 0.4060\n",
      "Epoch 46/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 2.4260 - accuracy: 0.3974\n",
      "Epoch 47/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 2.3494 - accuracy: 0.4188\n",
      "Epoch 48/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 2.2701 - accuracy: 0.4188\n",
      "Epoch 49/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2637 - accuracy: 0.4316\n",
      "Epoch 50/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.2953 - accuracy: 0.3932\n",
      "Epoch 51/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.1879 - accuracy: 0.4658\n",
      "Epoch 52/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 2.2084 - accuracy: 0.4188\n",
      "Epoch 53/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.1164 - accuracy: 0.4658\n",
      "Epoch 54/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 2.0587 - accuracy: 0.4829\n",
      "Epoch 55/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 2.0598 - accuracy: 0.4487\n",
      "Epoch 56/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.9839 - accuracy: 0.5171\n",
      "Epoch 57/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 2.0515 - accuracy: 0.4658\n",
      "Epoch 58/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.9578 - accuracy: 0.4786\n",
      "Epoch 59/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.9658 - accuracy: 0.4872\n",
      "Epoch 60/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.9994 - accuracy: 0.4701\n",
      "Epoch 61/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.9799 - accuracy: 0.5043\n",
      "Epoch 62/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.7947 - accuracy: 0.5342\n",
      "Epoch 63/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.8181 - accuracy: 0.5427\n",
      "Epoch 64/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.7818 - accuracy: 0.5385\n",
      "Epoch 65/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.7483 - accuracy: 0.5812\n",
      "Epoch 66/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.6752 - accuracy: 0.5940\n",
      "Epoch 67/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.7810 - accuracy: 0.5470\n",
      "Epoch 68/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.6647 - accuracy: 0.5427\n",
      "Epoch 69/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.5549 - accuracy: 0.6368\n",
      "Epoch 70/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.5643 - accuracy: 0.6197\n",
      "Epoch 71/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.6815 - accuracy: 0.5684\n",
      "Epoch 72/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.5482 - accuracy: 0.5983\n",
      "Epoch 73/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.6043 - accuracy: 0.5855\n",
      "Epoch 74/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.5066 - accuracy: 0.6282\n",
      "Epoch 75/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.4833 - accuracy: 0.5855\n",
      "Epoch 76/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.4636 - accuracy: 0.5940\n",
      "Epoch 77/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.4481 - accuracy: 0.6026\n",
      "Epoch 78/200\n",
      "5/5 [==============================] - 0s 30ms/step - loss: 1.4437 - accuracy: 0.6239\n",
      "Epoch 79/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.3426 - accuracy: 0.6624\n",
      "Epoch 80/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 1.4174 - accuracy: 0.6026\n",
      "Epoch 81/200\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 1.2898 - accuracy: 0.6581\n",
      "Epoch 82/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3293 - accuracy: 0.6624\n",
      "Epoch 83/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3765 - accuracy: 0.6453\n",
      "Epoch 84/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.3265 - accuracy: 0.6453\n",
      "Epoch 85/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 1.2995 - accuracy: 0.6496\n",
      "Epoch 86/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.3010 - accuracy: 0.6581\n",
      "Epoch 87/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2221 - accuracy: 0.7009\n",
      "Epoch 88/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.1889 - accuracy: 0.6838\n",
      "Epoch 89/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 1.2575 - accuracy: 0.6709\n",
      "Epoch 90/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1943 - accuracy: 0.7222\n",
      "Epoch 91/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.2619 - accuracy: 0.6709\n",
      "Epoch 92/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1485 - accuracy: 0.7009\n",
      "Epoch 93/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1025 - accuracy: 0.7350\n",
      "Epoch 94/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1217 - accuracy: 0.6880\n",
      "Epoch 95/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1835 - accuracy: 0.6880\n",
      "Epoch 96/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 1.1412 - accuracy: 0.7051\n",
      "Epoch 97/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1354 - accuracy: 0.7265\n",
      "Epoch 98/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 1.1264 - accuracy: 0.7051\n",
      "Epoch 99/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 1.1781 - accuracy: 0.6581\n",
      "Epoch 100/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1092 - accuracy: 0.6880\n",
      "Epoch 101/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.1450 - accuracy: 0.6838\n",
      "Epoch 102/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 1.0753 - accuracy: 0.7564\n",
      "Epoch 103/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 1.0857 - accuracy: 0.6752\n",
      "Epoch 104/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9707 - accuracy: 0.7265\n",
      "Epoch 105/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.9317 - accuracy: 0.7607\n",
      "Epoch 106/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.1122 - accuracy: 0.7009\n",
      "Epoch 107/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.9847 - accuracy: 0.7179\n",
      "Epoch 108/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 1.0559 - accuracy: 0.7051\n",
      "Epoch 109/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 1.0262 - accuracy: 0.7650\n",
      "Epoch 110/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9614 - accuracy: 0.7436\n",
      "Epoch 111/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.9379 - accuracy: 0.7778\n",
      "Epoch 112/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.9042 - accuracy: 0.7393\n",
      "Epoch 113/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8949 - accuracy: 0.7821\n",
      "Epoch 114/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8968 - accuracy: 0.7564\n",
      "Epoch 115/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8854 - accuracy: 0.7607\n",
      "Epoch 116/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.9071 - accuracy: 0.7821\n",
      "Epoch 117/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.9604 - accuracy: 0.7265\n",
      "Epoch 118/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9210 - accuracy: 0.7521\n",
      "Epoch 119/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.9298 - accuracy: 0.7051\n",
      "Epoch 120/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.8204 - accuracy: 0.7607\n",
      "Epoch 121/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8609 - accuracy: 0.7521\n",
      "Epoch 122/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.9292 - accuracy: 0.7821\n",
      "Epoch 123/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.8124 - accuracy: 0.7863\n",
      "Epoch 124/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.8704 - accuracy: 0.8034\n",
      "Epoch 125/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8937 - accuracy: 0.7564\n",
      "Epoch 126/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.9119 - accuracy: 0.7821\n",
      "Epoch 127/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.9265 - accuracy: 0.7350\n",
      "Epoch 128/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.7670 - accuracy: 0.8120\n",
      "Epoch 129/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.8846 - accuracy: 0.7692\n",
      "Epoch 130/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.7664 - accuracy: 0.8376\n",
      "Epoch 131/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6535 - accuracy: 0.8376\n",
      "Epoch 132/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6839 - accuracy: 0.8291\n",
      "Epoch 133/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.7592 - accuracy: 0.7906\n",
      "Epoch 134/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7595 - accuracy: 0.7906\n",
      "Epoch 135/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6633 - accuracy: 0.8034\n",
      "Epoch 136/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.7733 - accuracy: 0.7821\n",
      "Epoch 137/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.7339 - accuracy: 0.8291\n",
      "Epoch 138/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.7833 - accuracy: 0.7821\n",
      "Epoch 139/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.7264 - accuracy: 0.8034\n",
      "Epoch 140/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.8076 - accuracy: 0.7692\n",
      "Epoch 141/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.8086 - accuracy: 0.7735\n",
      "Epoch 142/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6980 - accuracy: 0.8034\n",
      "Epoch 143/200\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.7389 - accuracy: 0.8205\n",
      "Epoch 144/200\n",
      "5/5 [==============================] - 0s 23ms/step - loss: 0.7527 - accuracy: 0.8034\n",
      "Epoch 145/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6608 - accuracy: 0.8077\n",
      "Epoch 146/200\n",
      "5/5 [==============================] - 0s 19ms/step - loss: 0.6775 - accuracy: 0.8248\n",
      "Epoch 147/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6699 - accuracy: 0.8291\n",
      "Epoch 148/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6987 - accuracy: 0.8248\n",
      "Epoch 149/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6877 - accuracy: 0.8034\n",
      "Epoch 150/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5755 - accuracy: 0.8718\n",
      "Epoch 151/200\n",
      "5/5 [==============================] - 0s 18ms/step - loss: 0.6925 - accuracy: 0.7906\n",
      "Epoch 152/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6357 - accuracy: 0.8248\n",
      "Epoch 153/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.7717 - accuracy: 0.7821\n",
      "Epoch 154/200\n",
      "5/5 [==============================] - 0s 5ms/step - loss: 0.6373 - accuracy: 0.8504\n",
      "Epoch 155/200\n",
      "5/5 [==============================] - 0s 25ms/step - loss: 0.7619 - accuracy: 0.7564\n",
      "Epoch 156/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5770 - accuracy: 0.8291\n",
      "Epoch 157/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.6232 - accuracy: 0.8376\n",
      "Epoch 158/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.6067 - accuracy: 0.8504\n",
      "Epoch 159/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.6692 - accuracy: 0.8333\n",
      "Epoch 160/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6908 - accuracy: 0.8162\n",
      "Epoch 161/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.6443 - accuracy: 0.8291\n",
      "Epoch 162/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6068 - accuracy: 0.8547\n",
      "Epoch 163/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6204 - accuracy: 0.8291\n",
      "Epoch 164/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.6065 - accuracy: 0.8248\n",
      "Epoch 165/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.6203 - accuracy: 0.8547\n",
      "Epoch 166/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.6470 - accuracy: 0.8248\n",
      "Epoch 167/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5892 - accuracy: 0.8632\n",
      "Epoch 168/200\n",
      "5/5 [==============================] - 0s 13ms/step - loss: 0.7184 - accuracy: 0.7991\n",
      "Epoch 169/200\n",
      "5/5 [==============================] - 0s 27ms/step - loss: 0.6506 - accuracy: 0.8120\n",
      "Epoch 170/200\n",
      "5/5 [==============================] - 0s 21ms/step - loss: 0.5421 - accuracy: 0.8761\n",
      "Epoch 171/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5191 - accuracy: 0.8675\n",
      "Epoch 172/200\n",
      "5/5 [==============================] - 0s 17ms/step - loss: 0.5759 - accuracy: 0.8504\n",
      "Epoch 173/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5935 - accuracy: 0.8376\n",
      "Epoch 174/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.6086 - accuracy: 0.8462\n",
      "Epoch 175/200\n",
      "5/5 [==============================] - 0s 20ms/step - loss: 0.5813 - accuracy: 0.8376\n",
      "Epoch 176/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5109 - accuracy: 0.8675\n",
      "Epoch 177/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.6278 - accuracy: 0.8248\n",
      "Epoch 178/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5760 - accuracy: 0.8547\n",
      "Epoch 179/200\n",
      "5/5 [==============================] - 0s 9ms/step - loss: 0.5146 - accuracy: 0.8846\n",
      "Epoch 180/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.5182 - accuracy: 0.8462\n",
      "Epoch 181/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5878 - accuracy: 0.8333\n",
      "Epoch 182/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.5115 - accuracy: 0.8803\n",
      "Epoch 183/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.6260 - accuracy: 0.8248\n",
      "Epoch 184/200\n",
      "5/5 [==============================] - 0s 10ms/step - loss: 0.5492 - accuracy: 0.8419\n",
      "Epoch 185/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5786 - accuracy: 0.8547\n",
      "Epoch 186/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4562 - accuracy: 0.8803\n",
      "Epoch 187/200\n",
      "5/5 [==============================] - 0s 8ms/step - loss: 0.5586 - accuracy: 0.8291\n",
      "Epoch 188/200\n",
      "5/5 [==============================] - 0s 16ms/step - loss: 0.5446 - accuracy: 0.8462\n",
      "Epoch 189/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4735 - accuracy: 0.8590\n",
      "Epoch 190/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5107 - accuracy: 0.8462\n",
      "Epoch 191/200\n",
      "5/5 [==============================] - 0s 14ms/step - loss: 0.4585 - accuracy: 0.8846\n",
      "Epoch 192/200\n",
      "5/5 [==============================] - 0s 15ms/step - loss: 0.5860 - accuracy: 0.8248\n",
      "Epoch 193/200\n",
      "5/5 [==============================] - 0s 12ms/step - loss: 0.4877 - accuracy: 0.8718\n",
      "Epoch 194/200\n",
      "5/5 [==============================] - 0s 43ms/step - loss: 0.5402 - accuracy: 0.8291\n",
      "Epoch 195/200\n",
      "5/5 [==============================] - 0s 11ms/step - loss: 0.4762 - accuracy: 0.8761\n",
      "Epoch 196/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.5343 - accuracy: 0.8291\n",
      "Epoch 197/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4312 - accuracy: 0.8761\n",
      "Epoch 198/200\n",
      "5/5 [==============================] - 0s 7ms/step - loss: 0.4512 - accuracy: 0.8419\n",
      "Epoch 199/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.4993 - accuracy: 0.8803\n",
      "Epoch 200/200\n",
      "5/5 [==============================] - 0s 6ms/step - loss: 0.5488 - accuracy: 0.8376\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1eaca2123d0>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_x, train_y, epochs = 200, batch_size = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model2.pkl\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model2.pkl\\assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"model2.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
